services:
  server:
    build: ./backend
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_NAME
      - POSTGRES_HOST
      - POSTGRES_PORT
      - JWT_SECRET
      - KAFKA_URL=kafka:9092
    ports:
      - "3000:3000"
    depends_on:
      - postgres
    networks:
      - mynet
  outbox:
    build: ./backend
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_NAME
      - POSTGRES_HOST
      - POSTGRES_PORT
      - JWT_SECRET
      - KAFKA_URL=kafka:9092
    command: bun run outbox
    restart: unless-stopped
    depends_on:
      - postgres
      - kafka
    networks:
      - mynet

  init_db:
    build: ./backend
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_NAME
      - POSTGRES_HOST
      - POSTGRES_PORT
      - JWT_SECRET
      - KAFKA_URL=kafka:9092
    command: bun run init_db
    depends_on:
      - postgres
    networks:
      - mynet
  postgres:
    image: postgres:16
    container_name: postgres_db
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB=${POSTGRES_NAME}
    ports:
      - "5432:5432"
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "max_connections=300"
    # restart: unless-stopped
    networks:
      - mynet

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.4
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "22181:2181"
    networks:
      - mynet

  kafka:
    image: confluentinc/cp-kafka:7.4.4
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_PORT}:${KAFKA_PORT}"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:${KAFKA_PORT}
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:${KAFKA_PORT}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      # KAFKA_LOG_DIRS: /kafka/data
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - mynet

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    depends_on:
      - kafka
    ports:
      - "19000:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
    networks:
      - mynet

  jobmanager:
    build: ./analytics
    ports:
      - "8081:8081"
    command: jobmanager
    volumes:
      - flink_data:/tmp/
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        state.backend: hashmap
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 5000
        heartbeat.timeout: 60000
        rest.flamegraph.enabled: true
        web.backpressure.refresh-interval: 10000
    networks:
      - mynet

  taskmanager:
    build: ./analytics
    depends_on:
      - jobmanager
    command: taskmanager
    volumes:
      - flink_data:/tmp/
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 3
        state.backend: hashmap
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        heartbeat.interval: 5000
        heartbeat.timeout: 60000
    networks:
      - mynet

volumes:
  postgres_data:
  connectors:
  flink_data:

networks:
  mynet:
    driver: bridge
